<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Stanford University CS231n: Convolutional Neural Networks for Visual Recognition</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-46895817-2', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />

  <style>
  .hh {
    margin: 10px 0px 10px 0px;
    border-bottom: 1px solid #8C1515;
    color: #8C1515;
  }
  </style>
</head>

<body>

<div id="header">
  <a href="http://svl.stanford.edu/">
    <img src="img/SVLLogo.svg" style="height:50px; float: left; margin-left: 20px;">
  </a>
  <a href="http://stanford.edu/">
    <img src="img/stanfordlogo.jpg" style="height:50px; float: right; margin-right: 20px;">
  </a>

  <a href="index.html">
    <h1>CS231n: Convolutional Neural Networks for Visual Recognition</h1>
  </a>

  <div style="clear:both;"></div>
</div>

<div style="background-color:#8C1515; color:#FFF; padding:15px;">
<h1>Course Project</h1>
</div>

<!-- <div class="container sec">
  Warning: Details still subject to change
</div> -->

<div class="container sec">
  <h2>Overview</h2>
  <p>
    The Course Project is an opportunity for you to apply what you have learned in class to a
    problem of your interest. Potential projects usually fall into these two tracks:
  </p>
  <ul>
    <li>
      <strong>Applications.</strong>
      If you're coming to the class with a specific background and interests (e.g. biology,
      engineering, physics), we'd love to see you apply ConvNets to problems related to your
      particular domain of interest. Pick a real-world problem and apply ConvNets to solve it.
    </li>
    <li>
      <strong>Models.</strong>
      You can build a new model (algorithm) with ConvNets, or a new variant of existing models,
      and apply it to tackle vision tasks. This track might be more challenging, and sometimes
      leads to a piece of publishable work.
    </li>
  </ul>
  <p>
    One <b>restriction</b> to note is that this is a Computer Vision class, so your project should
    involve pixels of visual data in some form somewhere. E.g. a pure NLP project is not a good
    choice, even if your approach involves ConvNets.
  </p>
  <p>
    To get a better feeling for what we expect from CS231n projects, we encourage you to take a
    look at the project reports from previous years:
    <ul>
      <li><a href="http://cs231n.stanford.edu/2017/reports.html">Spring 2017</a></li>
      <li><a href="http://cs231n.stanford.edu/2016/reports.html">Winter 2016</a></li>
      <li><a href="http://cs231n.stanford.edu/2015/reports.html">Winter 2015</a></li>
    </ul>
  </p>
  <p>
    To inspire ideas, you might also look at recent deep learning publications from top-tier
    conferences, as well as other resources below.
  </p>
  <ul>
    <li>
      <a href="http://openaccess.thecvf.com/CVPR2017.py">CVPR</a>: 
      IEEE Conference on Computer Vision and Pattern Recognition
    </li>
    <li>
      <a href="http://openaccess.thecvf.com/ICCV2017.py">ICCV</a>:
      International Conference on Computer Vision
    </li>
    <li>
      <a href="http://www.eccv2016.org/main-conference/">ECCV</a>:
      European Conference on Computer Vision
    </li>
    <li>
      <a href="https://papers.nips.cc/">NIPS</a>: Neural Information Processing Systems
    </li>
    <li>
      <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference">ICLR</a>:
      International Conference on Learning Representations
    </li>
    <li>
      <a href="https://icml.cc/Conferences/2017/Schedule?type=Poster">ICML</a>:
      International Conference on Machine Learning
    </li>
    <li>
      Publications from the <a href="http://vision.stanford.edu/publications.html">Stanford Vision Lab</a>
    </li>
    <li><a href="https://github.com/kjw0612/awesome-deep-vision">Awesome Deep Vision</a></li>
    <li>
      <a href="http://cs229.stanford.edu/projects.html">Past CS229 Projects</a>:
      Example projects from Stanford's machine learning class
    </li>
    <li>
      <a href="http://www.kaggle.com/">Kaggle challenges</a>:
      An online machine learning competition website. For example, a
      <a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">Yelp classification challenge</a>.
    </li>
  </ul>
  <p>
    For applications, this type of projects would involve careful data preparation, an appropriate
    loss function, details of training and cross-validation and good test set evaluations and model
    comparisons. Don't be afraid to think outside of the box. Some successful examples can be found
    below:
  </p>
  <ul>
    <li><a href="http://arxiv.org/abs/1412.3409">Teaching Deep Convolutional Neural Networks to Play Go</a></li>
    <li><a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
    <li><a href="http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/">Winning the Galaxy Challenge with convnets</a>
    <!-- <li><a href="http://benanne.github.io/2014/08/05/spotify-cnns.html">Recommending music on Spotify with deep learning</a></li> -->
  </ul>
  <p>
    ConvNets also run in real time on mobile phones and Raspberry Pi's - building an interesting
    mobile application could be a good project. If you want to go this route you might want to
    check out <a href="https://www.tensorflow.org/mobile/">TensorFlow Mobile / Lite</a> or
    <a href="https://caffe2.ai/docs/mobile-integration.html">Caffe2 iOS/Android integration</a>.
  </p> 
  <p>
    For models, ConvNets have been successfully used in a variety of computer vision tasks. This
    type of projects would involve understanding the state-of-the-art vision models, and building
    new models or improving existing models for a vision task. The list below presents some papers
    on recent advances of ConvNets in the computer vision community.
  </p>
  <ul>
    <li>
      <strong>Image Classification</strong>:
      <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">[Krizhevsky et al.]</a>,
      <a href="http://arxiv.org/abs/1409.0575">[Russakovsky et al.]</a>,
      <a href="http://arxiv.org/abs/1409.4842">[Szegedy et al.]</a>,
      <a href="http://arxiv.org/abs/1409.1556">[Simonyan et al.]</a>,
      <a href="http://arxiv.org/abs/1406.4729">[He et al.]</a>,
      <a href="https://arxiv.org/abs/1608.06993">[Huang et al.]</a>,
      <a href="https://arxiv.org/abs/1709.01507">[Hu et al.]</a>
      <a href="https://arxiv.org/abs/1707.07012">[Zoph et al.]</a>
    </li>
    <li>
      <strong>Object detection</strong>:
      <a href="http://arxiv.org/abs/1311.2524">[Girshick et al.]</a>,
      <a href="https://arxiv.org/abs/1506.01497">[Ren et al.]</a>,
      <a href="https://arxiv.org/abs/1703.06870">[He et al.]</a>
    </li>
    <li><strong>Image segmentation</strong>: 
      <a href="http://arxiv.org/abs/1411.4038">[Long et al.]</a>
      <a href="https://arxiv.org/abs/1505.04366">[Noh et al.]</a>
      <a href="http://ieeexplore.ieee.org/abstract/document/7913730/">[Chen et al.]</a>
    </li>
    <li>
      <strong>Video classification</strong>:
      <a href="http://cs.stanford.edu/people/karpathy/deepvideo/">[Karpathy et al.]</a>,
      <a href="http://arxiv.org/abs/1406.2199">[Simonyan and Zisserman]</a>
      <a href="https://arxiv.org/abs/1412.0767">[Tran et al.]</a>
      <a href="https://arxiv.org/abs/1705.07750">[Carreira et al.]</a>
      <a href="https://arxiv.org/abs/1711.07971">[Wang et al.]</a>
    </li>
    <li><strong>Scene classification</strong>:
      <a href="http://places.csail.mit.edu/">[Zhou et al.]</a>
    </li>
    <li>
      <strong>Face recognition</strong>:
      <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[Taigman et al.]</a>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf">[Schroff et al.]</a>
      <a href="http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf">[Parkhi et al.]</a>
    </li>
    <li>
      <strong>Depth estimation</strong>:
      <a href="http://www.cs.nyu.edu/~deigen/depth/">[Eigen et al.]</a>
    </li>
    <li>
      <strong>Image-to-sentence generation</strong>:
      <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">[Karpathy and Fei-Fei]</a>,
      <a href="http://arxiv.org/abs/1411.4389">[Donahue et al.]</a>,
      <a href="http://arxiv.org/abs/1411.4555">[Vinyals et al.]</a>
      <a href="https://arxiv.org/pdf/1502.03044.pdf">[Xu et al.]</a>
      <a href="https://arxiv.org/abs/1511.07571">[Johnson et al.]</a>
    </li>
    <li>
      <strong>Visualization and optimization</strong>:
      <a href="http://arxiv.org/pdf/1312.6199v4.pdf">[Szegedy et al.]</a>,
      <a href="http://arxiv.org/abs/1412.1897">[Nguyen et al.]</a>,
      <a href="http://arxiv.org/abs/1311.2901">[Zeiler and Fergus]</a>,
      <a href="http://arxiv.org/abs/1412.6572">[Goodfellow et al.]</a>,
      <a href="http://arxiv.org/abs/1312.6055">[Schaul et al.]</a>
    </li>
  </ul>
  <p>
    You might also gain inspiration by taking a look at some popular computer vision datasets:
  </p>
  <p>
    <ul>
      <li><a href="http://www.cvpapers.com/datasets.html">Meta Pointer: A large collection organized by CV Datasets.</a></li>
      <li><a href="http://riemenschneider.hayko.at/vision/dataset/">Yet another Meta pointer</a></li>
      <li><a href="http://http://image-net.org/">ImageNet</a>: a large-scale image dataset for visual recognition organized by <a href="http://wordnet.princeton.edu/">WordNet</a> hierarchy</li>
      <li><a href="http://groups.csail.mit.edu/vision/SUN/">SUN Database</a>: a benchmark for scene recognition and object detection with annotated scene categories and segmented objects</li>
      <li><a href="http://places.csail.mit.edu/">Places Database</a>: a scene-centric database with 205 scene categories and 2.5 millions of labelled images</li>
      <li><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth Dataset v2</a>: a RGB-D dataset of segmented indoor scenes</li>
      <li><a href="http://mscoco.org/">Microsoft COCO</a>: a new benchmark for image recognition, segmentation and captioning</li>
      <li><a href="http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images">Flickr100M</a>: 100 million creative commons Flickr images</li>
      <li><a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a>: a dataset of 13,000 labeled face photographs</li>
      <li><a href="http://human-pose.mpi-inf.mpg.de/">Human Pose Dataset</a>: a benchmark for articulated human pose estimation</li>
      <li><a href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces DB</a>: a face video dataset for unconstrained face recognition in videos</li>
      <li><a href="http://crcv.ucf.edu/data/UCF101.php">UCF101</a>: an action recognition data set of realistic action videos with 101 action categories</li>
      <li><a href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB-51</a>: a large human motion dataset of 51 action classes</li>
      <li><a href="http://activity-net.org/">ActivityNet</a>: A large-scale video dataset for human activity understanding</li>
      <li><a href="http://moments.csail.mit.edu/">Moments in Time</a>: A dataset of one million 3-second videos</li>
    </ul>
  </p>

  <h2>Collaboration Policy</h2>
  <p>
    You can work in teams of up to <strong>3</strong> people. We do expect that projects done with
    3 people have more impressive writeup and results than projects done with 2 people. To get a
    sense for the scope and expectations for 2-people projects have a look at project reports from
    previous years.
  </p>

  <h2>Honor Code</h2>
  <p>
    You may consult any papers, books, online references, or publicly available implementations for
    ideas and code that you may want to incorporate into your strategy or algorithm, so long as you
    clearly cite your sources in your code and your writeup. However, under no circumstances may you
    look at another group’s code or incorporate their code into your project.
  </p>
  <p>
    If you are combining your course project with the project from another class, you must receive
    permission from the instructors, and clearly explain in the Proposal, Milestone, and Final Report
    the exact portion of the project that is being counted for CS 231n. In this case you must prepare
    separate reports for each course, and submit your final report for the other course as well.
  </p>

  <h2>Important Dates</h2>
  Unless otherwise noted, all project items are due by 11:59 pm Pacific Time.<br>
  <ul>
    <li>Project proposal: due Wednesday, April 24.</li>
    <li>Project milestone: due Wednesday, May 15.</li>
    <li>Final report: due Tuesday, June 4. <b>No late days.</b></li>
    <li>Poster PDF: due Monday, June 10. <b>No late days.</b></li>
    <li>Poster session: Tuesday, June 11.</li>
  </ul>

  <h2>Project Proposal</h2>
  <p>
    The project proposal should be one paragraph (200-400 words). Your project proposal should
    describe:
  </p>
  <ul>
    <li>What is the problem that you will be investigating? Why is it interesting?</li>
    <li>What reading will you examine to provide context and background?</li>
    <li>What data will you use? If you are collecting new data, how will you do it?</li>
    <li>
      What method or algorithm are you proposing? If there are existing implementations, will you
      use them and how? How do you plan to improve or modify such implementations? You don't have
      to have an exact answer at this point, but you should have a general sense of how you will
      approach the problem you are working on.
    </li>
    <li>
      How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g.
      plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or
      compare your results (e.g. what performance metrics or statistical tests)?
    </li>
  </ul>
  <p>
    <b>Submission:</b>
    Please submit your proposal as a PDF on Gradescope. <b>Only one person on your team should submit.</b>
    Please have this person add the rest of your team as collaborators as a "Group Submission".
  </p>

  <h2>Project Milestone</h2>
  Fine-grained requirements are listed on <a href="https://piazza.com/class/js3o5prh5w378a?cid=958">Piazza</a>.
  Your project milestone report should be between 2 - 3 pages using the
  <a href="http://cvpr2017.thecvf.com/files/cvpr2017AuthorKit.zip">provided template</a>.
  The following is a suggested structure for your report:
  <p>
    <ul>
      <li>Title, Author(s)</li>
      <li>Introduction: this section introduces your problem, and the overall plan for approaching your problem</li>
      <li>Problem statement: Describe your problem precisely specifying the dataset to be used, expected results and evaluation</li>
      <li>Technical Approach: Describe the methods you intend to apply to solve the given problem</li>
      <li>
      Intermediate/Preliminary Results: State and evaluate your results upto the milestone
      </li>
    </ul>
  </p>
  <p>
    <strong>Submission</strong>:
    Please submit your milestone as a PDF on Gradescope. <b>Only one person on your team should submit.</b>
    Please have this person add the rest of your team as collaborators as a "Group Submission".
  </p>

  <h2>Final Report</h2>
  <p>
    Your final write-up is required to be between <b>6 - 8</b> pages using the
    <a href="http://www.pamitc.org/cvpr15/files/cvpr2015AuthorKit.zip">provided template</a>,
    structured like a paper from a computer vision conference (CVPR, ECCV, ICCV, etc.).
    Please use this template so we can fairly judge all student projects without worrying about
    altered font sizes, margins, etc. After the class, we will post all the final reports online
    so that you can read about each others' work. If you do not want your writeup to be posted
    online, then please let us know via the project registration form.
  </p>
  <p>
    The following is a suggested structure for your report, as well as the rubric that we will
    follow when evaluating reports. You don't necessarily have to organize your report using
    these sections in this order, but that would likely be a good starting point for most projects.
    <br>Refer to <a href="https://piazza.com/class/js3o5prh5w378a?cid=1627">[Gradescope]</a> for more fine-grained details and explanations of each separate section.
  </p>
  <ul>
    <li><b>Title, Author(s)</b></li>
    <li>
      <b>Abstract</b>: Briefly describe your problem, approach, and key results. Should be no more
      than 300 words.
    </li>
    <li>
      <b>Introduction (10%)</b>:
      Describe the problem you are working on, why it's important, and an overview of your results
    </li>
    <li>
      <b>Related Work (10%)</b>:
      Discuss published work that relates to your project. How is your approach similar or different
      from others?
    </li>
    <li>
      <b>Data (10%)</b>:
      Describe the data you are working with for your project. What type of data is it? Where did it
      come from? How much data are you working with? Did you have to do any preprocessing, filtering,
      or other special treatment to use this data in your project?
    </li>
    <li>
      <b>Methods (30%)</b>:
      Discuss your approach for solving the problems that you set up in the introduction. Why is
      your approach the right thing to do? Did you consider alternative approaches? You should
      demonstrate that you have applied ideas and skills built up during the quarter to tackling
      your problem of choice. It may be helpful to include figures, diagrams, or tables to
      describe your method or compare it with other methods.
    </li>
    <li>
      <b>Experiments (30%)</b>:
      Discuss the experiments that you performed to demonstrate that your approach solves the
      problem. The exact experiments will vary depending on the project, but you might compare
      with previously published methods, perform an ablation study to determine the impact of
      various components of your system, experiment with different hyperparameters or architectural
      choices, use visualization techniques to gain insight into how your model works, discuss
      common failure modes of your model, etc. You should include graphs, tables, or other figures
      to illustrate your experimental results.
    </li>
    <li>
      <b>Conclusion (5%)</b>
      Summarize your key results - what have you learned? Suggest ideas for future extensions
      or new applications of your ideas.
    </li>
    <li>
      <b>Writing / Formatting (5%)</b>
      Is your paper clearly written and nicely formatted?
    </li>
    <li>
      <b>Supplementary Material</b>, not counted toward your 6-8 page limit and submitted as
      a separate file. Your supplementary material might include:
      <ul>
        <li>
          Source code (if your project proposed an algorithm, or code that is relevant and important
          for your project.).
        </li>
        <li>Cool videos, interactive visualizations, demos, etc.</li>
      </ul>
      Examples of things to not put in your supplementary material: 
      <ul>
        <li>The entire PyTorch/TensorFlow Github source code.</li>
        <li>Any code that is larger than 10 MB.</li>
        <li>Model checkpoints.</li>
        <li>A computer virus.</li>
      </ul>
    </li>
  </ul>
  <p>
    <b>Submission</b>:
    You will submit your final report as a PDF and your supplementary material as a separate PDF
    or ZIP file. We will provide detailed submission instructions as the deadline nears.
  </p>
  <p>
    <b>Additional Submission Requirements</b>:
    We will also ask you do do the following when you submit your project report:
  </p>
  <ul>
    <li>
      <b>Your report PDF should list <i>all</i> authors who have contributed to your work; enough to
        warrant a co-authorship position.</b> This includes people not enrolled in CS 231N such as
      faculty/advisors if they sponsored your work with funding or data, significant mentors (e.g.,
      PhD students or postdocs who coded with you, collected data with you, or helped draft your
      model on a whiteboard). All authors should be listed directly underneath the title on your PDF.
      Include a footnote on the first page indicating which authors are not enrolled in CS 231N. All
      co-authors should have their institutional/organizational affiliation specified below the title.
    </li>
    <ul>
      <li>If you have non-231N contributors, you will be asked to describe the following:</li>
      <li>
        <b>Specify the involvement of non-CS 231N contributors</b>
        (discussion, writing code, writing paper, etc). For an example, please see the author
        contributions for <a href="https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html#author-information" target="_blank">AlphaGo (Nature, 2016)</a>.
        <li>
          <b>Specify whether the project has been submitted to a peer-reviewed conference or journal.</b>
          Include the full name and acronym of the conference (if applicable). For example: Neural
          Information Processing Systems (NIPS). This only applies if you have already
          <i>submitted</i> your paper/manuscript and it is under review as of the report deadline.
      </li>
    </ul>
    <li>
      <b>Any code that was used as a base for projects must be referenced and cited in the body of the paper.</b>
      This includes CS 231N assignment code, finetuning example code, open-source, or Github
      implementations. You can use a footnote or full reference/bibliography entry.
    </li>
    <li>
      <b>If you are using this project for multiple classes, submit the other class PDF as well.</b>
      Remember, it is an honor code violation to use the same final report PDF for multiple classes.
    </li>
  </ul>
  <p>
    In summary, include all contributing authors in your PDF; include detailed non-231N co-author
    information; tell us if you submitted to a conference, cite any code you used, and submit your
    dual-project report (e.g., CS 230, CS 231A, CS 234).
  </p>
  
  <h2>Poster Session</h2>
  We will hold a poster session in which you will present the results of your projects is form of a poster.<br>
  <ul>
    <li><b>Date:</b> Tuesday, June 11, 2018</li>
    <li><b>Time:</b> 12:00 pm to 3:15 pm </li>
    <li><b>Location:</b> <a href="https://campus-map.stanford.edu/?id=08-450&lat=37.43075809&lng=-122.16485253&zoom=17&srch=arrillaga%20alumni%20center"> Arrillaga Alumni Center</a></li>
    <li>
      <b>Who:</b> All on-campus students are required to attend. Local SCPD students are highly
      recommended to attend. Stanford students, faculty, and guests from industry are welcome!
    </li>
    <li><b>Food:</b> Food and light refreshments will be provided.</li>
  </ul>
  <p>
    Students: We will provide foam poster boards and easels. Please print your poster on a 20 inch
    by 30 inch poster in either landscape or portrait format. Posters larger than 24 inch by 36
    inches may not fit on our poster boards.  All students are required to submit a PDF of their
    poster before the event. See Piazza for details. Caution: Do not wait until the day before the
    event to print your poster. Many on-campus printers (e.g., EE, BioX) run out of paper or toner
    during the last week of classes. Many other courses also have poster presentations or academic
    conferences take place during this week and there is no guarantee they will be able to
    rush-print your order.
  </p>

  <b>Frequently Asked Questions</b>
  <ul>
    <li>
      <b>I can only attend part of the poster session. Is that okay?</b>
      Yes, your team receives points for the poster session as long as one person from the team 
      attends and presents to the course staff. We will be taking attendance and 
      TAs/graders/instructors may visit your poster at any time.
    </li>
    <li>
      <b>At the event, can I leave my poster and walk around?</b> Yes. We encourage you to visit
      other posters and learn about the cutting-edge projects other students are working on.
      However, we do ask that you periodically return to your poster in case a TA or instructor
      needs to grade your poster.
    </li>
    <li>
      <b>I am a local SCPD student, is attendance required?</b> No. However, the poster session is
      an immensely valuable opportunity to network with on-campus students, the course staff, and
      various industry representatives (e.g., investors and recruiters). We strongly recommend you
      attend if possible. Many SCPD students cite the poster session as their favorite part of the
      class.
    </li>
    <li>
      <b>I am a non-local SCPD student and cannot attend. Do I have to make a poster?</b> 
      Yes. You are required to submit your poster as a PDF to Gradescope with the same deadline as
      on-campus students. We may require you to record a video of yourself presenting or conduct
      a presentation over video/conference call at a different time; check Piazza for details
      closer to the event.
    </li>
    <li>
      <b>Can I print my poster on 8.5"x11" pieces of paper and tape it together?</b> 
      Yes - we will not deduct points if you choose to do this. However we recommend you print a
      full-sized poster if possible. Not only can you fit more content on a poster, but the visual
      appeal can help attract visitors and spur additional research discussions about your project.
    </li>
    <li>
      <b>Will you reimburse for poster printing costs?</b>
      Unfortunately no. Several departments at Stanford offer free or discounted poster printing to
      students. Many local businesses (e.g., Staples, FedEx, etc.) offer same-day printing services
      at reasonable prices.
      <a href="https://vptl.stanford.edu/student-resources/computers-printing/poster-printing-lathrop" target="_blank">Lathrop Library</a>
      offers on-campus poster printing services.
    </li>
    <li>
      <b>I'm part of an organization and we'd like to sponsor or help contribute to the event. How can we get involved?</b>
      Please send an email to the course staff at cs231n-spring1819-staff@lists.stanford.edu. We have several sponsorship levels available.
    </li>
  </ul>
</div>

<!-- jQuery and Boostrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
